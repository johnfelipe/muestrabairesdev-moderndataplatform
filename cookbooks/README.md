# Libros de recetas de la plataforma de datos moderna - v1.16.0

Aquí estamos documentando libros de cocina sobre cómo usar la plataforma:

*   **Corriente de aire**
    *   [Programe y ejecute una aplicación Python simple](./recipes/airflow-schedule-python-app/README.md) - `1.12.0`

*   **Trino (anteriormente Presto SQL)**
    *   [Trino, Spark y Delta Lake (Spark 2.4.7 y Delta Lake 0.6.1)](./recipes/delta-lake-and-trino-spark2.4/README.md) - `1.11.0`
    *   [Trino, Spark y Delta Lake (Spark 3.0.1 y Delta Lake 0.7.0)](./recipes/delta-lake-and-trino-spark3.0/README.md) - `1.11.0`
    *   [Consulta de datos de S3 (MinIO) mediante MinIO](./recipes/querying-minio-with-trino/README.md) - `1.11.0`
    *   [Consulta de datos de Azure Data Lake Storage Gen2 (ADLS) desde Trino](./recipes/querying-adls-with-trino/README.md) - `1.15.0`
    *   [Consulta de datos en Postgresql desde Trino](./recipes/querying-postgresql-with-trino/README.md) - `1.11.0`
    *   [Consulta de datos en Kafka desde Trino (anteriormente PrestoSQL)](./recipes/querying-kafka-with-trino/README.md) - `1.14.0`
    *   [Consulta de datos HDFS mediante Trino](./recipes/querying-hdfs-with-presto/README.md) - `1.11.0`
    *   Unión de datos entre RDBMS y MinIO

*   **MQTT**
    *   [Uso de proxy MQTT confluente](./recipes/using-mqtt-proxy/README.md)
    *   [Uso de HiveMQ con extensiones de Kafka](./recipes/using-hivemq-with-kafka-extension/README.md) - `1.12.0`

*   **Chispa**
    *   [Ejecute la aplicación Java Spark usando `spark-submit`](./recipes/run-spark-simple-app-java-submit/README.md)
    *   [Ejecutar la aplicación Java Spark con Docker](./recipes/run-spark-simple-app-java-docker/README.md)
    *   [Ejecute la aplicación Scala Spark usando `spark-submit`](./recipes/run-spark-simple-app-scala-submit/README.md)
    *   [Ejecutar la aplicación Scala Spark con Docker](./recipes/run-spark-simple-app-scala-docker/README.md)
    *   [Ejecute la aplicación Python Spark usando `spark-submit`](./recipes/run-spark-simple-app-python-submit/README.md)
    *   [Ejecutar la aplicación Python Spark con Docker](./recipes/run-spark-simple-app-python-docker/README.md)
    *   [Spark y Hive Metastore](./recipes/spark-and-hive-metastore/README.md) - `1.15.0`
    *   [Spark con S3 interno (usando en minIO)](./recipes/spark-with-internal-s3/README.md)
    *   [Spark con S3 externo](./recipes/spark-with-external-s3/README.md)
    *   [Spark con PostgreSQL](./recipes/spark-with-postgresql/README.md) - `1.15.0`

*   **Hadoop HDFS**
    *   [Consulta de datos HDFS mediante Presto](./recipes/querying-hdfs-with-presto/README.md)
    *   [Uso de datos HDFS con Spark Data Frame](./recipes/using-hdfs-with-spark/README.md)

*   **Livy**
    *   [Enviar solicitud de Spark a través de Livio](./recipes/run-spark-simple-app-scala-livy/README.md)

*   **Recopilador de datos StreamSets**
    *   [Compatibilidad con la activación del recopilador de datos StreamSets](./recipes/streamsets-oss-activation/README.md) - `1.13.0`
    *   [Consumir un archivo binario y enviarlo como mensaje de Kafka](./recipes/streamsets-binary-file-to-kafka/README.md)
    *   [Uso de Dev Simulator Origin para simular datos de streaming](./recipes/using-dev-simulator-origin/README.md) - `1.12.0`
    *   [Carga de tuberías de Streamsets al inicio del contenedor](./recipes/streamsets-loading-pipelines/README.md) - `1.14.0`

*   **Plataforma DataOps de StreamSets**
    *   [Creación de un entorno de DataOps de StreamSets autogestionado mediante Platys](./recipes/streamsets-dataops-creating-environment/README.md) - `1.14.0`

*   **Kafka**
    *   [Configuración simulada de varios CC en una máquina](./recipes/simulated-multi-dc-setup/README.md) - `1.14.0`
    *   [Automatice la gestión de los temas de Kafka en la plataforma](./recipes/jikkou-automate-kafka-topics-management/README.md) - `1.14.0`

*   **Plataforma empresarial confluente**
    *   [Uso del almacenamiento de información en niveles empresarial confluente](./recipes/confluent-tiered-storage/README.md) - `1.13.0`

*   **ksqlDB**
    *   [Conexión a través de la CLI de ksqlDB](./recipes/connecting-through-ksqldb-cli/README.md)
    *   [UDF y ksqlDB personalizados](./recipes/custom-udf-and-ksqldb/README.md)

*   **Kafka Conectar**
    *   [Uso de kafka Connect Connector adicional](./recipes/using-additional-kafka-connect-connector/README.md)
    *   [Uso de un conector Kafka Connect que no esté en confluentes](./recipes/using-kafka-connector-not-in-confluent-hub/README.md) - `1.14.0`

*   **Registro Apicurio**
    *   [Registro apicurio con almacenamiento SQL (PostgreSQL)](./recipes/apicurio-with-database-storage/README.md) - `1.14.0`

*   **Jikkou**
    *   [Automatice la gestión de los temas de Kafka en la plataforma](./recipes/jikkou-automate-kafka-topics-management/README.md)

*   **Oracle RDBMS**
    *   [Uso de la imagen privada (Trivadis) de Oracle EE](./recipes/using-private-oracle-ee-image/README.md) - `1.13.0`
    *   [Uso de la imagen pública de Oracle XE](./recipes/using-public-oracle-xe-image/README.md) - `1.13.0`

*   **Neo4J**
    *   [Trabajar con Neo4J](./recipes/working-with-neo4j/README.md) - `1.15.0`

*   **Tablero de propinas**
    *   [ Trabajar con Tipboard y Kafka](./recipes/tipboard-and-kafka/README.md) - `1.14.0`

*   **Registros de decisión de arquitectura (ADR)**
    *   [Creación y visualización de ADR con log4brains](./recipes/creating-adr-with-log4brains/README.md) - `1.12.0`
